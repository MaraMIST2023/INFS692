X <- setdiff(names(rad_train), Y)
#Convert to categorical
#Y <- rad$Failure.binary
#Y <- to_categorical(Y, 180)
#Convert to matrix
#rad_mat <- as.matrix(rad)
#class(rad_mat)
#Create model
model <- keras_model_sequential() %>%
layer_dense(units = 256, activation = "sigmoid", input_shape = ncol(X)) %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 128, activation = "sigmoid") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 128, activation = "sigmoid") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 64, activation = "sigmoid") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 64, activation = "sigmoid") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 2, activation = "softmax")
#backpropagate
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
#compile
model %>% compile(
loss = "catrgorical_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
history <- model %>%
fit(
x = X,
y = Y,
epochs = 10,
batch_size = 128,
validation_split = 0.15,
verbose = FALSE
)
#Create model
model <- keras_model_sequential() %>%
layer_dense(units = 256, activation = "sigmoid", input_shape = ncol(X)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 128, activation = "sigmoid") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 128, activation = "sigmoid") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 64, activation = "sigmoid") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 64, activation = "sigmoid") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 2, activation = "softmax")
#backpropagate
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
#compile
model %>% compile(
loss = "catrgorical_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
history <- model %>%
fit(
x = X,
y = Y,
epochs = 10,
batch_size = 128,
validation_split = 0.15,
verbose = FALSE
)
#Identify features
Y <- "Failure.binary"
X <- setdiff(names(rad_train), Y)
#Convert to categorical
Y <- rad$Failure.binary
Y <- to_categorical(Y, 180)
#Convert to matrix
#rad_mat <- as.matrix(rad)
#class(rad_mat)
history <- model %>%
fit(
x = X,
y = Y,
epochs = 10,
batch_size = 128,
validation_split = 0.15,
verbose = FALSE
)
#checking size...
str(rad_train)
str(rad_test)
#checking size...
str(rad_train)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(caret)
library(rsample)
library(recipes)
library(dplyr)
library(ggplot2)
library(vip)
#Load the dataset
radData <- read.csv("radiomics_completedata.csv")
str(radData)
#check first entries
head(radData)
#null and missing values
na.omit(radData)
#normalize
rad <- radData %>% mutate_if(is.ordered, factor, ordered = FALSE)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
blueprint <- recipe(Failure.binary ~ ., data = rad_train) %>%
step_other(all_nominal(), threshold = 0.005)
h2o.init()
train.h2o <- prep(blueprint, training = rad_train, retain = TRUE) %>%
juice() %>%
as.h2o()
test.h2o <- prep(blueprint, training = rad_train) %>%
bake(new_data = rad_test) %>%
as.h2o()
Y <- "Failure.binary"
X <- setdiff(names(rad_train), Y)
#training matrices
#X <- model.matrix(Failure.binary ~ ., rad_train)[, -1]
#log transformation
#Y <- log(rad_train$Failure.binary)
#lasso <- cv.glmnet(
#  x = X,
#  y = Y,
#  alpha = 1
#)
set.seed(24)
model1 <- train(
Failure.binary ~ .,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
#Correlation
cor(x, y,  method = "pearson", use = "complete.obs")
#Correlation
cor(X, Y,  method = "pearson", use = "complete.obs")
XN <- as.numeric(X)
#Correlation
cor(XN, Y,  method = "pearson", use = "complete.obs")
#Correlation
cor(XN, YN,  method = "pearson", use = "complete.obs")
YN <- as.numeric(Y)
#Correlation
cor(XN, YN,  method = "pearson", use = "complete.obs")
# correlation for all variables
round(cor(rad),
digits = 2 # rounded to 2 decimals
)
# correlation for all variables
cor(rad),
# correlation for all variables
cor(rad)
# correlation for all variables
round(cor(XN),
digits = 2 # rounded to 2 decimals
)
# correlation for all variables
round(cor(XN, YN),
digits = 2 # rounded to 2 decimals
)
cor(rad)
#check first entries
head(radData)
cor(radData)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(caret)
library(rsample)
library(recipes)
library(dplyr)
library(ggplot2)
library(vip)
#Load the dataset
radData <- read.csv("radiomics_completedata.csv")
str(radData)
#check first entries
head(radData)
#null and missing values
na.omit(radData)
#normalize
rad <- radData %>% mutate_if(is.ordered, factor, ordered = FALSE)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
blueprint <- recipe(Failure.binary ~ ., data = rad_train) %>%
step_other(all_nominal(), threshold = 0.005)
h2o.init()
train.h2o <- prep(blueprint, training = rad_train, retain = TRUE) %>%
juice() %>%
as.h2o()
test.h2o <- prep(blueprint, training = rad_train) %>%
bake(new_data = rad_test) %>%
as.h2o()
Y <- "Failure.binary"
YN <- as.numeric(Y)
X <- setdiff(names(rad_train), Y)
XN <- as.numeric(X)
#Correlation
cor(XN, YN,  method = "pearson", use = "complete.obs")
set.seed(24)
model1 <- train(
Failure.binary ~ .,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
model1 <- train(
rad$Failure.binary ~ .,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
View(rad)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
blueprint <- recipe(Failure.binary ~ ., data = rad_train) %>%
step_other(all_nominal(), threshold = 0.005)
Y <- "Failure.binary"
YN <- as.numeric(Y)
X <- setdiff(names(rad_train), Y)
XN <- as.numeric(X)
#Correlation
#cor(XN, YN,  method = "pearson", use = "complete.obs")
#training matrices
#X <- model.matrix(Failure.binary ~ ., rad_train)[, -1]
#log transformation
#Y <- log(rad_train$Failure.binary)
#lasso <- cv.glmnet(
#  x = X,
#  y = Y,
#  alpha = 1
#)
set.seed(24)
model1 <- train(
rad$Failure.binary ~ .,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
#train and validate glm
rad_glm <- h2o.glm(
x = X, y = Y, training_frame = train.h2o, alpha = 0.1,
remove_collinear_columns = TRUE, nfolds = 10, fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE, seed = 24
)
library(ROCR)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
Y <- "Failure.binary"
YN <- as.numeric(Y)
X <- setdiff(names(rad_train), Y)
XN <- as.numeric(X)
#Correlation
#cor(XN, YN,  method = "pearson", use = "complete.obs")
#training matrices
#X <- model.matrix(Failure.binary ~ ., rad_train)[, -1]
#log transformation
#Y <- log(rad_train$Failure.binary)
#lasso <- cv.glmnet(
#  x = X,
#  y = Y,
#  alpha = 1
#)
set.seed(24)
model1 <- train(
rad$Failure.binary ~ .,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
set.seed(24)
model1 <- train(
Failure.binary ~ .,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
model1 <- train(
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
model1 <- train(
rad ~ Failure.binary,
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
model1 <- train(
rad ~ "Failure.binary",
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
library(glmnet)
library(caret)
library(rsample)
library(ROCR)
library(dplyr)
library(ggplot2)
library(vip)
#Load the dataset
radData <- read.csv("radiomics_completedata.csv")
str(radData)
#check first entries
head(radData)
#null and missing values
na.omit(radData)
#normalize
rad <- radData %>% mutate_if(is.ordered, factor, ordered = FALSE)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
Y <- "Failure.binary"
YN <- as.numeric(Y)
X <- setdiff(names(rad_train), Y)
XN <- as.numeric(X)
#Correlation
#cor(XN, YN,  method = "pearson", use = "complete.obs")
#training matrices
#X <- model.matrix(Failure.binary ~ ., rad_train)[, -1]
#log transformation
#Y <- log(rad_train$Failure.binary)
#lasso <- cv.glmnet(
#  x = X,
#  y = Y,
#  alpha = 1
#)
set.seed(24)
model1 <- train(
rad ~ "Failure.binary",
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
library(randomForest)
library(glmnet)
library(caret)
library(ROCR)
library(dplyr)
library(ggplot2)
library(vip)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(caret)
library(ROCR)
library(dplyr)
library(ggplot2)
library(vip)
#Load the dataset
radData <- read.csv("radiomics_completedata.csv")
str(radData)
#check first entries
head(radData)
#null and missing values
na.omit(radData)
#normalize
#rad <- radData %>% mutate_if(is.ordered, factor, ordered = FALSE)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
Y <- "Failure.binary"
YN <- as.numeric(Y)
X <- setdiff(names(rad_train), Y)
XN <- as.numeric(X)
#Correlation
#cor(XN, YN,  method = "pearson", use = "complete.obs")
#training matrices
#X <- model.matrix(Failure.binary ~ ., rad_train)[, -1]
#log transformation
#Y <- log(rad_train$Failure.binary)
#lasso <- cv.glmnet(
#  x = X,
#  y = Y,
#  alpha = 1
#)
set.seed(24)
model1 <- train(
rad ~ "Failure.binary",
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
#Load the dataset
radData <- read.csv("radiomics_completedata.csv")
str(radData)
#check first entries
head(radData)
#null and missing values
na.omit(radData)
#normalize
rad <- radData %>% mutate_if(is.ordered, factor, ordered = FALSE)
#Split data into training (80%) and testing (20%)
set.seed(24)
rad_split <- initial_split(rad, prop = 0.8, strata = "Failure.binary")
rad_train <- training(rad_split)
rad_test <- testing(rad_split)
Y <- "Failure.binary"
YN <- as.numeric(Y)
X <- setdiff(names(rad_train), Y)
XN <- as.numeric(X)
#Correlation
#cor(XN, YN,  method = "pearson", use = "complete.obs")
#training matrices
#X <- model.matrix(Failure.binary ~ ., rad_train)[, -1]
#log transformation
#Y <- log(rad_train$Failure.binary)
#lasso <- cv.glmnet(
#  x = X,
#  y = Y,
#  alpha = 1
#)
testData <- rad[-trainIndex, ]
set.seed(123)
trainIndex <- createDataPartition(rad$Failure.binary, p = 0.8, list = FALSE)
trainData <- rad[trainIndex, ]
testData <- rad[-trainIndex, ]
set.seed(24)
model1 <- train(
rad ~ "Failure.binary",
data = rad_train,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10)
)
lrModel <- train(Species ~ ., data = trainData, method = "glm", family = "binomial")
lrModel <- train(Failure.binary ~ ., data = trainData, method = "glm", family = "binomial")
library(keras)
set.seed(123)
trainIndex <- createDataPartition(rad$Failure.binary, p = 0.8, list = FALSE)
trainData <- rad[trainIndex, ]
testData <- rad[-trainIndex, ]
lrModel <- train(Failure.binary ~ ., data = trainData, method = "glm", family = "binomial")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(ClusterR)
library(cluster)
library(h2o)
library(readr)
library(mclust)
#PCA
rad_basket <- readr::read_csv("radiomics_completedata.csv")
dim(rad_basket)
h2o.init()
rad_basket.h2o <- as.h2o(rad_basket)
rad_pca <- h2o.prcomp(
training_frame = rad_basket.h2o,
pca_method = "GramSVD",
k = 197,
transform = "STANDARDIZE",
impute_missing = TRUE,
max_runtime_secs = 1000
)
summary(rad_pca)
#seed
set.seed(24)
init <- sample(3, nrow(Y), replace = TRUE)
#hierarchical cluster
radHC <- hclust(dist(init))
radHC
#K-means and hierarchical clustering are heuristic-base algorithms that create groupings based on the data provided with no measure of probability or uncertainty.
#model cluster
radMB <- Mclust(init)
radMB
#In model-based clustering, observations have a probability of belonging to each group. These methods of unsupervised learning can be compared and evaluated using visualization and and mathematical techniques such as plotting, or comparing metrics such as RMSE.
#K-means
radK <- kmeans(init, centers = 2, nstart = 20)
str(radK)
#cluster each observation
radK$cluster
init <- sample(3, nrow(Y), replace = TRUE)
init <- sample(2, nrow(Y), replace = TRUE)
init <- sample(3, nrow(Y), replace = TRUE)
